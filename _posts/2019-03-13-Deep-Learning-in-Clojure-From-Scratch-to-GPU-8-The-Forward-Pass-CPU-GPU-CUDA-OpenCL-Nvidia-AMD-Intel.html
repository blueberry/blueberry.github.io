---
date: 2019-03-13
tags:
- Deep Learning
- Neanderthal
- MKL
- Clojure
- CUDA
- OpenCL
author: dragan
layout: post
title: Deep Learning from Scratch to GPU - 8 - The Forward Pass (CUDA, OpenCL, Nvidia, AMD, Intel)
excerpt: We start implementing stochastic gradient descent and the backpropagation algorithm. Here we implement the forward pass of the training layer and run it on the CPU and GPU.
categories:
- Deep Learning
- Neanderthal
- MKL
- Clojure
- CUDA
- OpenCL
---
<p>
We start implementing stochastic gradient descent and the backpropagation algorithm.
Here we implement the forward pass of the training layer and run it on the CPU and GPU.
</p>

<p>
If you haven't yet, read my introduction to this series in <a href="./Deep-Learning-in-Clojure-From-Scratch-to-GPU-0-Why-Bother">Deep Learning in Clojure from Scratch to GPU - Part 0 - Why Bother?</a>.
</p>

<p>
The previous article, Part 7, is here: <a href="./Deep-Learning-in-Clojure-From-Scratch-to-GPU-7-Learning%20and%20Backpropagation">Learning and Backpropagation</a>.
</p>

<p>
To run the code, you need a Clojure project with <a href="https://neanderthal.uncomplicate.org">Neanderthal</a> (<iframe class="github-btn" src="https://ghbtns.com/github-btn.html?user=uncomplicate&amp;repo=neanderthal&amp;type=watch&amp;count=true" width="100" height="20" title="Star on GitHub" frameBorder="0"></iframe>) included as a dependency.
If you're in a hurry, you can clone <a href="https://github.com/uncomplicate/neanderthal/blob/master/examples/hello-world/">Neanderthal Hello World project</a>.
</p>

<p>
Don't forget to read at least some introduction from <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a>,
start up the REPL from your favorite Clojure development environment, and let's continue with the tutorial.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>require '<span style="color: #7388d6;">[</span>uncomplicate.commons.core
           <span style="color: #F5666D;">:refer</span> <span style="color: #909183;">[</span>with-release <span style="color: #A52A2A; font-weight: bold;">let-release</span> Releaseable release<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span>
         '<span style="color: #7388d6;">[</span>uncomplicate.clojurecuda.core <span style="color: #F5666D;">:as</span> cuda
           <span style="color: #F5666D;">:refer</span> <span style="color: #909183;">[</span>current-context default-stream synchronize!<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span>
         '<span style="color: #7388d6;">[</span>uncomplicate.clojurecl.core <span style="color: #F5666D;">:as</span> opencl
           <span style="color: #F5666D;">:refer</span> <span style="color: #909183;">[</span><span style="color: #0084C8; font-weight: bold;">*context*</span> <span style="color: #0084C8; font-weight: bold;">*command-queue*</span> finish!<span style="color: #909183;">]</span><span style="color: #7388d6;">]</span>
         '<span style="color: #7388d6;">[</span>uncomplicate.neanderthal
           <span style="color: #909183;">[</span>core <span style="color: #F5666D;">:refer</span> <span style="color: #709870;">[</span>mrows dim raw view axpy! copy! scal! transfer!
                         transfer mm! rk! view-ge vctr ge entry!<span style="color: #709870;">]</span><span style="color: #909183;">]</span>
           <span style="color: #909183;">[</span>native <span style="color: #F5666D;">:refer</span> <span style="color: #709870;">[</span>native-float<span style="color: #709870;">]</span><span style="color: #909183;">]</span>
           <span style="color: #909183;">[</span>vect-math <span style="color: #F5666D;">:refer</span> <span style="color: #709870;">[</span>tanh! linear-frac!<span style="color: #709870;">]</span><span style="color: #909183;">]</span>
           <span style="color: #909183;">[</span>cuda <span style="color: #F5666D;">:refer</span> <span style="color: #709870;">[</span>cuda-float<span style="color: #709870;">]</span><span style="color: #909183;">]</span>
           <span style="color: #909183;">[</span>opencl <span style="color: #F5666D;">:refer</span> <span style="color: #709870;">[</span>opencl-float<span style="color: #709870;">]</span><span style="color: #909183;">]</span><span style="color: #7388d6;">]</span><span style="color: #707183;">)</span>
<span style="color: #707183;">(</span>import 'clojure.lang.IFn<span style="color: #707183;">)</span>
</pre>
</div>

<div id="outline-container-org523dece" class="outline-2">
<h2 id="org523dece">The network diagram</h2>
<div class="outline-text-2" id="text-org523dece">
<p>
I'm repeating the network diagram from the <a href="./Deep-Learning-in-Clojure-From-Scratch-to-GPU-3-Fully-Connected-Inference-Layers">previous article</a> as a convenient reference.
</p>


<div class="figure">
<p><img src="../img/deep-learning-from-scratch/1/nn-bias-activation.png" alt="nn-bias-activation.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org53abc9e" class="outline-2">
<h2 id="org53abc9e">The relevant equations</h2>
<div class="outline-text-2" id="text-org53abc9e">
<p>
The layer itself computes the linear transformation \(z^l\), and its non-linear activation \(a^l\).
</p>

<p>
\(a^l = \sigma(z^l)\), where \(z^l = w^l a^{l-1} + b^l\).
</p>

<p>
Then, in the backward pass, we need the result of the linear transformation \(z^l\) to compute the updates.
</p>

<p>
In the output layer:
</p>

<p>
\(\delta^L = \nabla_a C \odot \sigma ' (z^L)\).
</p>

<p>
In other layers:
</p>

<p>
\(\delta^l =  ((w^{l+1})^T \delta^{l+1}) \odot \sigma ' (z^L)\).
</p>

<p>
Note that in both of these formulas, we compute the Haddamard product of a matrix
that comes from the next layer (\(\nabla_a C\) and \((w^{l+1})^T \delta^{l+1}\)) and
the matrix that is located in the layer at hand (\(\sigma ' (z^L)\)).
</p>
</div>
</div>

<div id="outline-container-orgb176b1b" class="outline-2">
<h2 id="orgb176b1b">The inference layer</h2>
<div class="outline-text-2" id="text-orgb176b1b">
<p>
The existing layer type performs all computations that we need for the forward pass.
However, it does not remember data that will be useful in the backward pass. Let's examine it
and see what we need to fix.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">defprotocol</span> <span style="color: #2F8B58; font-weight: bold;">Parameters</span>
  <span style="color: #7388d6;">(</span>weights <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>bias <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>

<span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">defprotocol</span> <span style="color: #2F8B58; font-weight: bold;">ActivationProvider</span>
  <span style="color: #7388d6;">(</span>activation-fn <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">deftype</span> <span style="color: #2F8B58; font-weight: bold;">FullyConnectedInference</span> <span style="color: #7388d6;">[</span>w b activ-fn<span style="color: #7388d6;">]</span>
  Releaseable
  <span style="color: #7388d6;">(</span>release <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span>release w<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>release b<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  Parameters
  <span style="color: #7388d6;">(</span>weights <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> w<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>bias <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> b<span style="color: #7388d6;">)</span>
  ActivationProvider
  <span style="color: #7388d6;">(</span>activation-fn <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> activ-fn<span style="color: #7388d6;">)</span>
  IFn
  <span style="color: #7388d6;">(</span>invoke <span style="color: #909183;">[</span>_ x ones a<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span>activ-fn <span style="color: #709870;">(</span>rk! -1.0 b ones <span style="color: #907373;">(</span>mm! 1.0 w x 0.0 a<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Instead of keeping \(z^l\) around, we overwrite it by using <code>a</code> for both the linear output
and, later, the activation.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">defn</span> <span style="color: #00578E; font-weight: bold;">fully-connected</span> <span style="color: #7388d6;">[</span>factory activ-fn in-dim out-dim<span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span><span style="color: #A52A2A; font-weight: bold;">let-release</span> <span style="color: #909183;">[</span>w <span style="color: #709870;">(</span>ge factory out-dim in-dim<span style="color: #709870;">)</span>
                bias <span style="color: #709870;">(</span>vctr factory out-dim<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span>-&gt;FullyConnectedInference w bias activ-fn<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgbdfbeef" class="outline-2">
<h2 id="orgbdfbeef">The training layer</h2>
<div class="outline-text-2" id="text-orgbdfbeef">
<p>
In the inference layer, we didn't have a need for outputs once the signal passes the layer.
Now, we do. First, we need to keep \(z^l\) around. As the relevant equations suggest,
we also need access to the output of the previous layer, \(a^{l-1}\), and the error propagated from
the subsequent layers.
</p>

<p>
We will work out the details in the following articles, but, for now,
we can see that each <i>training</i> layer should have its own reference to <code>z</code>.
</p>

<p>
We create the <code>Backprop</code> protocol with methods for moving forward and backward,
and for accessing the activation. Since, by design, all layers in the network operate on
the same batch size, we can reuse the vector of ones by propagating it from each layer to the next
during the construction.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">defprotocol</span> <span style="color: #2F8B58; font-weight: bold;">Backprop</span>
  <span style="color: #7388d6;">(</span>forward <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>backward <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">defprotocol</span> <span style="color: #2F8B58; font-weight: bold;">Transfer</span>
  <span style="color: #7388d6;">(</span>input <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>output <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>ones <span style="color: #909183;">[</span>this<span style="color: #909183;">]</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
The major novelty in <code>FullyConnectedTraining</code> is how we treat the input and output matrices.
In the inference layer implementation, these were function arguments unrelated to the layer
object. In the training layer, they become part of the layer.
</p>

<p>
Instead of implementing <code>invoke</code>, we implement <code>forward</code>, while most other differences
are related to bookkeeping.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">deftype</span> <span style="color: #2F8B58; font-weight: bold;">FullyConnectedTraining</span> <span style="color: #7388d6;">[</span>w b a-1 z a ones-vctr activ-fn<span style="color: #7388d6;">]</span>
  Releaseable
  <span style="color: #7388d6;">(</span>release <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span>release w<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>release b<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>release a-1<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>release z<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>release a<span style="color: #909183;">)</span>
    <span style="color: #909183;">(</span>release ones<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  Parameters
  <span style="color: #7388d6;">(</span>weights <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> w<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>bias <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> b<span style="color: #7388d6;">)</span>
  Transfer
  <span style="color: #7388d6;">(</span>input <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> a-1<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>output <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> a<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>ones <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span> ones-vctr<span style="color: #7388d6;">)</span>
  Backprop
  <span style="color: #7388d6;">(</span>forward <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span>activ-fn <span style="color: #709870;">(</span>rk! -1.0 b ones-vctr <span style="color: #907373;">(</span>mm! 1.0 w a-1 0.0 z<span style="color: #907373;">)</span><span style="color: #709870;">)</span> a<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>backward <span style="color: #909183;">[</span>_<span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span><span style="color: #A52A2A; font-weight: bold;">throw</span> <span style="color: #709870;">(</span>ex-info <span style="color: #4E9A06;">"</span><span style="color: #cc9393;">TODO</span><span style="color: #4E9A06;">"</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
The activation function is not allowed to overwrite the argument, so we
create a two-argument version. The input <code>x</code> (<code>z</code>) is unchanged, while the output <code>y</code> (<code>a</code>)
is overwritten with the result.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">defn</span> <span style="color: #00578E; font-weight: bold;">sigmoid!</span>
  <span style="color: #7388d6;">(</span><span style="color: #909183;">[</span>x<span style="color: #909183;">]</span>
   <span style="color: #909183;">(</span>linear-frac! 0.5 <span style="color: #709870;">(</span>tanh! <span style="color: #907373;">(</span>scal! 0.5 x<span style="color: #907373;">)</span><span style="color: #709870;">)</span> 0.5<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #909183;">[</span>x y<span style="color: #909183;">]</span>
   <span style="color: #909183;">(</span>linear-frac! 0.5 <span style="color: #709870;">(</span>tanh! <span style="color: #907373;">(</span>scal! 0.5 <span style="color: #6276ba;">(</span>copy! x y<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span> 0.5<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
Constructors have some interesting details. I decided that the training layer is going
to be wrapped around an inference layer, a sort of an "attachment" to it. Once the training layer
trains the parameters, we can dispose it, and continue using the lighter inference layer,
without it knowing how these weights and biases were determined.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #A52A2A; font-weight: bold;">defn</span> <span style="color: #00578E; font-weight: bold;">training-layer</span>
  <span style="color: #7388d6;">(</span><span style="color: #909183;">[</span>inference-layer input ones-vctr<span style="color: #909183;">]</span>
   <span style="color: #909183;">(</span><span style="color: #A52A2A; font-weight: bold;">let-release</span> <span style="color: #709870;">[</span>w <span style="color: #907373;">(</span>view <span style="color: #6276ba;">(</span>weights inference-layer<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
                 b <span style="color: #907373;">(</span>view <span style="color: #6276ba;">(</span>bias inference-layer<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
                 a-1 <span style="color: #907373;">(</span>view input<span style="color: #907373;">)</span>
                 z <span style="color: #907373;">(</span>ge w <span style="color: #6276ba;">(</span>mrows w<span style="color: #6276ba;">)</span> <span style="color: #6276ba;">(</span>dim ones-vctr<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span>
                 a <span style="color: #907373;">(</span>raw z<span style="color: #907373;">)</span>
                 o <span style="color: #907373;">(</span>view ones-vctr<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
     <span style="color: #709870;">(</span>-&gt;FullyConnectedTraining w b a-1 z a o <span style="color: #907373;">(</span>activation-fn inference-layer<span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span><span style="color: #909183;">[</span>inference-layer previous-backprop<span style="color: #909183;">]</span>
   <span style="color: #909183;">(</span>training-layer inference-layer
                   <span style="color: #709870;">(</span>output previous-backprop<span style="color: #709870;">)</span>
                   <span style="color: #709870;">(</span>ones previous-backprop<span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<p>
<code>w</code> and <code>b</code> are just views of the same underlying memory from the inference layer.
<a href="https://neanderthal.uncomplicate.org/codox/uncomplicate.neanderthal.core.html#var-view"><code>view</code></a> is a polymorphic Neanderthal function that creates a default Neanderthal structure
that reuses the underlying memory of the argument. In this case, we use it to create
additional instances of matrices, that operate on the same data, while having a separate life-cycle.
Releasing views does not release the buffers in the "master" structure.
</p>

<p>
The reference <code>a-1</code> is a view that can read and write data from <code>a</code> of the previous
layer, but when we release <code>a-1</code>, that does not affect the previous layer. Of course, if we released
the previous layer, the layer at hand will raise an error if we tried to use its <code>a-1</code>.
</p>

<p>
<code>z</code> and <code>a</code> are master properties of this layer. They have the same dimensions. <code>ones</code> is also a view.
</p>
</div>
</div>

<div id="outline-container-orgcef5771" class="outline-2">
<h2 id="orgcef5771">Creating and using the layers</h2>
<div class="outline-text-2" id="text-orgcef5771">
<p>
We are still using the low-level API that directly creates layers. Continuing with the same
example from the previous articles, we create two inference layers, but now we create training
layers for each. Instead of invoking inference layers as functions, now we invoke the forward method
on training layers. At the end, we read the output from the last layer's activation.
</p>

<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span>with-release <span style="color: #7388d6;">[</span>x <span style="color: #909183;">(</span>ge native-float 2 2 <span style="color: #709870;">[</span>0.3 0.9 0.3 0.9<span style="color: #709870;">]</span><span style="color: #909183;">)</span>
               ones <span style="color: #909183;">(</span>vctr native-float 1 1<span style="color: #909183;">)</span>
               layer-1 <span style="color: #909183;">(</span>fully-connected native-float tanh! 2 4<span style="color: #909183;">)</span>
               layer-2 <span style="color: #909183;">(</span>fully-connected native-float sigmoid! 4 1<span style="color: #909183;">)</span>
               training-layer-1 <span style="color: #909183;">(</span>training-layer layer-1 x ones<span style="color: #909183;">)</span>
               training-layer-2 <span style="color: #909183;">(</span>training-layer layer-2 training-layer-1<span style="color: #909183;">)</span><span style="color: #7388d6;">]</span>
  <span style="color: #7388d6;">(</span>transfer! <span style="color: #909183;">[</span>0.3 0.1 0.9 0.0 0.6 2.0 3.7 1.0<span style="color: #909183;">]</span> <span style="color: #909183;">(</span>weights layer-1<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>transfer! <span style="color: #909183;">[</span>0.7 0.2 1.1 2<span style="color: #909183;">]</span> <span style="color: #909183;">(</span>bias layer-1<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>transfer! <span style="color: #909183;">[</span>0.75 0.15 0.22 0.33<span style="color: #909183;">]</span> <span style="color: #909183;">(</span>weights layer-2<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>transfer! <span style="color: #909183;">[</span>0.3<span style="color: #909183;">]</span> <span style="color: #909183;">(</span>bias layer-2<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>forward training-layer-1<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>forward training-layer-2<span style="color: #7388d6;">)</span>
  <span style="color: #7388d6;">(</span>transfer <span style="color: #909183;">(</span>output training-layer-2<span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
nil#RealGEMatrix[float, mxn:1x2, layout:column, offset:0]
   ▥       ↓       ↓       ┓
   →       0.44    0.44
   ┗                       ┛
</pre>


<p>
Hey, this is the same result as before. Good job (for now :)
</p>

<p>
I think it would be a good idea to leave something easy for exercise. Following the
<a href="./Deep-Learning-in-Clojure-From-Scratch-to-GPU-6-CUDA-and-OpenCL">earlier article</a>  and the upcoming benchmarking code for CUDA and OpenCL, you can test yourself whether this code
works correctly on the GPU.
</p>

<p>
The required memory in training layers increased in regard to plain inference layers.
We now have to keep both <code>z</code> and <code>a</code>, which takes twice the number of neurons in the layer times batch size.
</p>
</div>
</div>

<div id="outline-container-orge0dc8c9" class="outline-2">
<h2 id="orge0dc8c9">Micro benchmark</h2>
<div class="outline-text-2" id="text-orge0dc8c9">
<p>
Having tested the correctness of what we have implemented in this article, I think
it would be a good idea to also test the speed. Does a slight change in having to keep more data around
affect the speed of computations?
</p>
</div>

<div id="outline-container-org4f66f7d" class="outline-3">
<h3 id="org4f66f7d">Nvidia GTX 1080 Ti (2017)</h3>
<div class="outline-text-3" id="text-org4f66f7d">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #2F8B58; font-weight: bold;">cuda</span>/with-default
  <span style="color: #7388d6;">(</span>with-release <span style="color: #909183;">[</span>factory <span style="color: #709870;">(</span>cuda-float <span style="color: #907373;">(</span>current-context<span style="color: #907373;">)</span> default-stream<span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span>with-release <span style="color: #709870;">[</span>x <span style="color: #907373;">(</span>ge factory 10000 10000<span style="color: #907373;">)</span>
                   ones <span style="color: #907373;">(</span>entry! <span style="color: #6276ba;">(</span>vctr factory 10000<span style="color: #6276ba;">)</span> 1<span style="color: #907373;">)</span>
                   layer-1 <span style="color: #907373;">(</span>fully-connected factory tanh! 10000 5000<span style="color: #907373;">)</span>
                   layer-2 <span style="color: #907373;">(</span>fully-connected factory sigmoid! 5000 1000<span style="color: #907373;">)</span>
                   layer-3 <span style="color: #907373;">(</span>fully-connected factory sigmoid! 1000 10<span style="color: #907373;">)</span>
                   training-layer-1 <span style="color: #907373;">(</span>training-layer layer-1 x ones<span style="color: #907373;">)</span>
                   training-layer-2 <span style="color: #907373;">(</span>training-layer layer-2 training-layer-1<span style="color: #907373;">)</span>
                   training-layer-3 <span style="color: #907373;">(</span>training-layer layer-3 training-layer-2<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span>time
       <span style="color: #907373;">(</span><span style="color: #A52A2A; font-weight: bold;">do</span>
         <span style="color: #6276ba;">(</span>forward training-layer-1<span style="color: #6276ba;">)</span>
         <span style="color: #6276ba;">(</span>forward training-layer-2<span style="color: #6276ba;">)</span>
         <span style="color: #6276ba;">(</span>forward training-layer-3<span style="color: #6276ba;">)</span>
         <span style="color: #6276ba;">(</span>synchronize!<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
niltrue
</pre>


<pre class="example">
"Elapsed time: 119.121297 msecs"
</pre>

<p>
Saving intermediate results did not increase running time. That's good news!
</p>
</div>
</div>

<div id="outline-container-orgb651cdc" class="outline-3">
<h3 id="orgb651cdc">AMD R9 290X (2013)</h3>
<div class="outline-text-3" id="text-orgb651cdc">
<div class="org-src-container">
<pre class="src src-clojure"><span style="color: #707183;">(</span><span style="color: #2F8B58; font-weight: bold;">opencl</span>/with-default
  <span style="color: #7388d6;">(</span>with-release <span style="color: #909183;">[</span>factory <span style="color: #709870;">(</span>opencl-float <span style="color: #0084C8; font-weight: bold;">*context*</span> <span style="color: #0084C8; font-weight: bold;">*command-queue*</span><span style="color: #709870;">)</span><span style="color: #909183;">]</span>
    <span style="color: #909183;">(</span>with-release <span style="color: #709870;">[</span>x <span style="color: #907373;">(</span>ge factory 10000 10000<span style="color: #907373;">)</span>
                   ones <span style="color: #907373;">(</span>entry! <span style="color: #6276ba;">(</span>vctr factory 10000<span style="color: #6276ba;">)</span> 1<span style="color: #907373;">)</span>
                   layer-1 <span style="color: #907373;">(</span>fully-connected factory tanh! 10000 5000<span style="color: #907373;">)</span>
                   layer-2 <span style="color: #907373;">(</span>fully-connected factory sigmoid! 5000 1000<span style="color: #907373;">)</span>
                   layer-3 <span style="color: #907373;">(</span>fully-connected factory sigmoid! 1000 10<span style="color: #907373;">)</span>
                   training-layer-1 <span style="color: #907373;">(</span>training-layer layer-1 x ones<span style="color: #907373;">)</span>
                   training-layer-2 <span style="color: #907373;">(</span>training-layer layer-2 training-layer-1<span style="color: #907373;">)</span>
                   training-layer-3 <span style="color: #907373;">(</span>training-layer layer-3 training-layer-2<span style="color: #907373;">)</span><span style="color: #709870;">]</span>
      <span style="color: #709870;">(</span>forward training-layer-1<span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span>finish!<span style="color: #709870;">)</span>
      <span style="color: #709870;">(</span>time
       <span style="color: #907373;">(</span><span style="color: #A52A2A; font-weight: bold;">do</span>
         <span style="color: #6276ba;">(</span>forward training-layer-1<span style="color: #6276ba;">)</span>
         <span style="color: #6276ba;">(</span>forward training-layer-2<span style="color: #6276ba;">)</span>
         <span style="color: #6276ba;">(</span>forward training-layer-3<span style="color: #6276ba;">)</span>
         <span style="color: #6276ba;">(</span>finish!<span style="color: #6276ba;">)</span><span style="color: #907373;">)</span><span style="color: #709870;">)</span><span style="color: #909183;">)</span><span style="color: #7388d6;">)</span><span style="color: #707183;">)</span>
</pre>
</div>

<pre class="example">
"Elapsed time: 337.34171 msecs"
</pre>

<p>
Compared to 337 ms we got <a href="./Deep-Learning-in-Clojure-From-Scratch-to-GPU-6-CUDA-and-OpenCL">before</a>, we see that there is no slowdown in the OpenCL implementation either.
</p>
</div>
</div>
</div>

<div id="outline-container-orgdce8da2" class="outline-2">
<h2 id="orgdce8da2">Donations</h2>
<div class="outline-text-2" id="text-orgdce8da2">
<p>
If you feel that you can afford to help, and wish to donate, I even created a special <i>Starbucks for two</i> tier
at <a href="https://www.patreon.com/draganrocks">patreon.com/draganrocks</a>. You can do something even cooler: <a href="https://www.patreon.com/posts/22476035">adopt a pet function</a>.
</p>
</div>
</div>

<div id="outline-container-org4f718e1" class="outline-2">
<h2 id="org4f718e1">The next article</h2>
<div class="outline-text-2" id="text-org4f718e1">
<p>
In the next article, we will see what should be improved in the current implementation to enable
the backward pass. As it is easy, we'll continue running everything on CPU, an Nvidia GPU and an AMD GPU.
</p>
</div>
</div>

<div id="outline-container-orgd70231c" class="outline-2">
<h2 id="orgd70231c">Thank you</h2>
<div class="outline-text-2" id="text-orgd70231c">
<p>
<a href="https://www.clojuriststogether.org/news/q1-2019-funding-announcement/">Clojurists Together</a> financially supported writing this series. Big thanks to all Clojurians who contribute,
and thank you for reading and discussing this series.
</p>
</div>
</div>
